{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.12.19 Çarşamba Kadir Berat YILDIRIM\n",
    "\n",
    "# Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87874334 0.91764332 0.92939993 0.81481783 0.89529628]\n",
      "average score 0.89\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "data = pd.read_csv(\"Advertising.csv\", index_col = 0)\n",
    "model_elasticnet = linear_model.ElasticNet(alpha = 0.1, l1_ratio = 0.5)\n",
    "features = [\"TV\",\"Radio\",\"Newspaper\"]\n",
    "x = data[features]\n",
    "y = data.Sales\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model_elasticnet,x,y,cv=5)\n",
    "print (scores)\n",
    "print (\"average score %.2f\" %scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /home/hign/scikit_learn_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5863093677864049\n",
      "alpha 0.06\n",
      "l1~l ratio 0.50\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "#import sklearn.cross_validation as cv ### deprecated, used sklearn.model_selection instead!\n",
    "import sklearn.model_selection as cv\n",
    "from sklearn import linear_model\n",
    "\n",
    "dataset = sklearn.datasets.fetch_california_housing()\n",
    "X = dataset['data']\n",
    "y = dataset['target']\n",
    "X_train, X_test, y_train, y_test = cv.train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "model = linear_model.ElasticNetCV(cv = 5)\n",
    "model.fit(X, y)\n",
    "model_score = model.score(X, y)\n",
    "print (model_score)\n",
    "print (\"alpha %.2f\" % model.alpha_)\n",
    "print (\"l1~l ratio %.2f\" % model.l1_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting models with Gradient Descent\n",
    "\n",
    "Linear model fitted by minimizing a regularized empirical loss with SGD\n",
    "\n",
    "SGD stands for Stochastic Gradient Descent: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate).\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to discriminative learning of linear classifiers under convex loss functions such as (linear) Support Vector Machines and Logistic Regression. Even though SGD has been around in the machine learning community for a long time, it has received a considerable amount of attention just recently in the context of large-scale learning.\n",
    "\n",
    "SGD has been successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing. Given that the data is sparse, the classifiers in this module easily scale to problems with more than 10^5 training examples and more than 10^5 features.\n",
    "\n",
    "The advantages of Stochastic Gradient Descent are:\n",
    "\n",
    "        - Efficiency.\n",
    "\n",
    "        - Ease of implementation (lots of opportunities for code tuning).\n",
    "\n",
    "The disadvantages of Stochastic Gradient Descent include:\n",
    "\n",
    "        - SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.\n",
    "\n",
    "        - SGD is sensitive to feature scaling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[10, 10], [20, 20], [30, 30], [40, 40]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 10],\n",
       "       [20, 20],\n",
       "       [30, 30],\n",
       "       [40, 40]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25., 25.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled = scaler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.34164079, -1.34164079],\n",
       "       [-0.4472136 , -0.4472136 ],\n",
       "       [ 0.4472136 ,  0.4472136 ],\n",
       "       [ 1.34164079,  1.34164079]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scaled.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scaled.std(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD example on Boston housing data set\n",
    "\n",
    "First scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "#from sklearn.cross_validation import cross_val_score ### Deprecated. Used model_selection instead!\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.model_selection import train_test_split ### Deprecated. Used model_selection instead!\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target)\n",
    "\n",
    "#Standardize features by removing the mean and scaling to unit variance\n",
    "X_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "X_train = X_scaler.fit_transform(X_train)\n",
    "y_train = y_scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "X_test = X_scaler.transform(X_test)\n",
    "y_test = y_scaler.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After scaling, apply SGD regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation r-squared scores :  [0.76027886 0.71173076 0.73798963 0.64682739 0.62230817]\n",
      "Average cross validation r-squared score :  0.6958269621473854\n",
      "Test set r-squared score :  0.7504507329107704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/site-packages/sklearn/utils/validation.py:744: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/lib/python3.8/site-packages/sklearn/utils/validation.py:744: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/lib/python3.8/site-packages/sklearn/utils/validation.py:744: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/lib/python3.8/site-packages/sklearn/utils/validation.py:744: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/lib/python3.8/site-packages/sklearn/utils/validation.py:744: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/lib/python3.8/site-packages/sklearn/utils/validation.py:744: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "regressor = SGDRegressor(loss = 'squared_loss')\n",
    "scores = cross_val_score(regressor, X_train, y_train, cv = 5)\n",
    "print('Cross validation r-squared scores : ', scores)\n",
    "print('Average cross validation r-squared score : ', np.mean(scores))\n",
    "regressor.fit(X_train, y_train)\n",
    "print('Test set r-squared score : ', regressor.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.linear_model.SGDRegressor(loss='squared_loss', penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, warm_start=False, average=False)\n",
    "\n",
    "- eta0double\n",
    "\n",
    "     The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules. The default value is 0.01. \n",
    "\n",
    "- lossstr, default: ‘squared_loss’\n",
    "\n",
    "    The loss function to be used. The possible values are ‘squared_loss’, ‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’\n",
    "\n",
    "    The ‘squared_loss’ refers to the ordinary least squares fit. ‘huber’ modifies ‘squared_loss’ to focus less on getting outliers correct by switching from squared to     linear loss past a distance of epsilon. ‘epsilon_insensitive’ ignores errors less than epsilon and is linear past that; this is the loss function used in SVR. ‘squared_epsilon_insensitive’ is the same but becomes squared loss past a tolerance of epsilon.\n",
    "\n",
    "- alphafloat\n",
    "\n",
    "    Constant that multiplies the regularization term. Defaults to 0.0001 Also used to compute learning_rate when set to ‘optimal’.\n",
    "\n",
    "- l1_ratiofloat\n",
    "\n",
    "    The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Defaults to 0.15.\n",
    "\n",
    "- fit_interceptbool\n",
    "\n",
    "    Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True.\n",
    "\n",
    "- max_iterint, optional (default=1000)\n",
    "\n",
    "    The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the fit method, and not the partial_fit method.\n",
    "    \n",
    "- random_stateint, RandomState instance or None, optional (default=None)\n",
    "\n",
    "    The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "\n",
    "- learning_ratestring, optional\n",
    "\n",
    "    The learning rate schedule:\n",
    "\n",
    "    ‘constant’:\n",
    "        eta = eta0\n",
    "        \n",
    "    ‘optimal’:\n",
    "        eta = 1.0 / (alpha . (t + t0)) where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
    "        \n",
    "    ‘invscaling’: [default]\n",
    "        eta = eta0 / pow(t, power_t)\n",
    "        \n",
    "    ‘adaptive’:\n",
    "        eta = eta0, as long as the training keeps decreasing. Each time n_iter_no_change consecutive epochs fail to decrease the training loss by tol or fail to increase validation score by tol if early_stopping is True, the current learning rate is divided by 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5440802 , -0.32965085, -0.1026456 ,  0.23071016,  1.86689395,\n",
       "        0.27163436, -0.40207811,  1.33616677, -0.71849566, -0.57749762])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27.65171626, 19.58287617, 21.6792524 , 24.75776765, 39.86780104,\n",
       "       25.13569955, 18.91401588, 34.96657573, 15.99192377, 17.29402998,\n",
       "       33.46270381, 33.87789087, 14.94548063, 34.16295655, 17.77095582,\n",
       "       22.20258791, 22.11142774, 34.00624429, 22.99386777, 24.71141603,\n",
       "       31.10706043, 19.93328817, 16.30219596, 12.33606954, 15.54490523,\n",
       "       12.40992601, 13.25569815,  2.23719114, 21.93647418, 17.01792384,\n",
       "       21.43813619, 31.47550899, 10.22709822, 24.11851722, 24.16691725,\n",
       "       43.73370067, 11.24230557, 24.66108095, 13.83880048, 30.89739891,\n",
       "       24.68192498, 32.7319721 , 23.86634259, 31.90402329, 23.9598446 ,\n",
       "       16.19121412, 22.87228884, 22.3247217 , 28.45565244, 17.72878874,\n",
       "       13.80750868, 19.75971752, 14.47624882, 35.05088027, 23.82684195,\n",
       "       35.74710997, 21.98472834, 24.7449706 , 24.06863741, 31.97980351,\n",
       "        8.6280417 , 35.53914861, 23.85707244, 23.3830414 , 19.182666  ,\n",
       "       34.63777904, 24.75566697, 40.93413315, 28.45434759, 26.12762661,\n",
       "       22.71785441, 33.82454257, 13.73246014, 34.83242143, 16.69275235,\n",
       "        5.12191536, 22.26867394, 20.05436681, 21.9345094 , 11.82080228,\n",
       "       15.95578807, 12.90332629, 16.6093922 , 22.58200963, 20.26145609,\n",
       "       20.95389887, 12.29055823, 25.40538002, 15.78471535,  4.8815379 ,\n",
       "       22.16480299, 20.08563456, 19.63090208, 28.47212172, 26.89984875,\n",
       "       17.19506927, 26.03661456, 14.05944004,  7.0469077 , 21.10399633,\n",
       "       28.43796002, 33.50637743,  9.55900257,  6.65468865, 15.86576567,\n",
       "       29.21706475, 17.30442679, 12.11124748, 28.41851877, 33.53477006,\n",
       "       28.39688015,  3.95244066, 19.64341152, 21.37689796, -1.50318952,\n",
       "       31.41769376, 19.67108756,  9.17257302, 31.70887282, 19.62896149,\n",
       "       19.12673574,  3.1859572 , 20.05011778, 19.66313433, -7.60816493,\n",
       "       15.89290935, 37.94459143])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scaler.inverse_transform(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"credit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Income</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.891</td>\n",
       "      <td>3606</td>\n",
       "      <td>283</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>106.025</td>\n",
       "      <td>6645</td>\n",
       "      <td>483</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Asian</td>\n",
       "      <td>903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>104.593</td>\n",
       "      <td>7075</td>\n",
       "      <td>514</td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>148.924</td>\n",
       "      <td>9504</td>\n",
       "      <td>681</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>55.882</td>\n",
       "      <td>4897</td>\n",
       "      <td>357</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>16</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   Income  Limit  Rating  Cards  Age  Education  Gender Student  \\\n",
       "0           1   14.891   3606     283      2   34         11    Male      No   \n",
       "1           2  106.025   6645     483      3   82         15  Female     Yes   \n",
       "2           3  104.593   7075     514      4   71         11    Male      No   \n",
       "3           4  148.924   9504     681      3   36         11  Female      No   \n",
       "4           5   55.882   4897     357      2   68         16    Male      No   \n",
       "\n",
       "  Married  Ethnicity  Balance  \n",
       "0     Yes  Caucasian      333  \n",
       "1     Yes      Asian      903  \n",
       "2      No      Asian      580  \n",
       "3      No      Asian      964  \n",
       "4     Yes  Caucasian      331  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score is 0.000461 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[529.53623188, 509.80310881]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.get_dummies(data.Gender).values\n",
    "y = data.Balance.values.reshape(-1, 1)\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "model = linear_model.LinearRegression(fit_intercept = False)\n",
    "model.fit(x, y)\n",
    "\n",
    "print('model score is %f ' % model.score(x, y))\n",
    "\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean scores is 0.950422\n"
     ]
    }
   ],
   "source": [
    "data_regression = pd.get_dummies(data, columns = ['Ethnicity', 'Student', 'Gender', 'Married'])\n",
    "y = data.Balance.values.reshape(-1, 1)\n",
    "del data_regression['Balance']\n",
    "x = data_regression.applymap(pd.to_numeric)\n",
    "from sklearn import linear_model\n",
    "model = linear_model.LinearRegression()\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "rkf = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = True)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, x, y, cv = rkf)\n",
    "print('mean scores is %f' % scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'Income',\n",
       " 'Limit',\n",
       " 'Rating',\n",
       " 'Cards',\n",
       " 'Age',\n",
       " 'Education',\n",
       " 'Ethnicity_African American',\n",
       " 'Ethnicity_Asian',\n",
       " 'Ethnicity_Caucasian',\n",
       " 'Student_No',\n",
       " 'Student_Yes',\n",
       " 'Gender_Female',\n",
       " 'Gender_Male',\n",
       " 'Married_No',\n",
       " 'Married_Yes']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data_regression.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "- Preproccessing class offers polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quadratic polynomial score is 0.981642\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold as kf\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "#training data\n",
    "X_train = [[6], [8], [10], [14], [18]]\n",
    "y_train = [[7], [9], [13], [17.5], [18]]\n",
    "X_test = [[6], [8], [11], [16]]\n",
    "y_test = [[0], [12], [15], [10]]\n",
    "\n",
    "quadratic_featurizer = PolynomialFeatures(degree = 2)\n",
    "X_train_quadratic = quadratic_featurizer.fit_transform(X_train)\n",
    "X_test_quadratic = quadratic_featurizer.transform(X_test)\n",
    "regressor_quadratic = LinearRegression()\n",
    "regressor_quadratic.fit(X_train_quadratic, y_train)\n",
    "print (\"quadratic polynomial score is %f\" % regressor_quadratic.score(X_train_quadratic, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [3, -0.5, 2, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [2.5, 0.0, 2, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [3, -0.5, 2, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [2.5, 0.0, 2, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
